{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Projet AARN"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1. Importation des librairies"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/abdennour/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     /home/abdennour/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import re\n","from bs4 import BeautifulSoup\n","import email\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","import string\n","from nltk.corpus import stopwords\n","import tensorflow as tf\n","#import CounterVectorizer\n","#import Counter class\n","from collections import Counter\n","\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2. Importation des données"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["eham_folder_path = \"./data/easy_ham/\"\n","hham_folder_path = \"./data/hard_ham/\"\n","spam_folder_path = \"./data/spam_2/\"\n","\n","def getEmails(folder_path: str):\n","    data = []\n","    for fileName in os.listdir(folder_path):\n","        file =open(folder_path+fileName, encoding = \"ISO-8859-1\")\n","        text =file.read()\n","        data.append(text)\n","        file.close()\n","    return np.array(data)\n","eham = getEmails(eham_folder_path)\n","hham = getEmails(hham_folder_path)\n","spam = getEmails(spam_folder_path)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3. traitement des données (preprocessing)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.1. prendre le corps du mail \n","\n","    en utilisant la bibliothèque email, nous pouvons analyser l'email et en obtenir le corps (ce dont nous avons besoin)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def get_body(text_list: np.array):\n","    list_body = []\n","    for i, text in enumerate(text_list):\n","        text = email.message_from_string(text)\n","        if text.is_multipart():\n","            for part in text.walk():\n","                ctype = part.get_content_type()\n","                cdispo = str(part.get('Content-Disposition'))\n","                    # skip any text/plain (txt) attachments\n","                if ctype == 'text/plain' and 'attachment' not in cdispo:\n","                    body = part.get_payload(decode=True)  # get body of email\n","                    break\n","        else :\n","            body = text.get_payload(decode=True)\n","        list_body.append(body)\n","    return np.array(list_body)\n","eham = get_body(eham)\n","hham = get_body(hham)\n","spam = get_body(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.2. Suppression des balises HTML\n","\n","    Nous avons utilisé la bibliothèque beatifulsoup pour analyser la structure html (si elle existe) du corps et supprimer toutes les balises html."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_1312/1476071293.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  soup = BeautifulSoup(text, 'html.parser')\n","/tmp/ipykernel_1312/1476071293.py:4: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  soup = BeautifulSoup(text, 'html.parser')\n","Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n","Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n","Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n","Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"]}],"source":["def suprimer_HTML(text_list: np.array):\n","    list_html = []\n","    for i, text in enumerate(text_list):\n","        soup = BeautifulSoup(text, 'html.parser')\n","        list_html.append(soup.get_text())\n","    return np.array(list_html)\n","eham = suprimer_HTML(eham)\n","hham = suprimer_HTML(hham)\n","spam = suprimer_HTML(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.3. Minusculisation"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def minuscule(text_list: np.array):\n","    list_mins = []\n","    for i, text in enumerate(text_list):\n","        list_mins.append( text.lower())\n","    return np.array(list_mins)\n","eham = minuscule(eham)\n","hham = minuscule(hham)\n","spam = minuscule(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.4. Elimination des URLs et des adresses email\n","\n","    Nous avons utilisé la bibliothèque re pour supprimer les URLs et les adresses email."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def normaliser_url_email(text_list: np.array):\n","    text_url = []\n","    for i, text in enumerate(text_list):\n","        #supprimer les emails\n","        text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', 'emailaddr ', text)\n","        #supprimer les urls\n","        link_pattern=r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))';\n","        text = re.sub(link_pattern, 'httpaddr ', text)\n","        text_url.append(text)\n","    return np.array(text_url)\n","eham = normaliser_url_email(eham)\n","hham = normaliser_url_email(hham)\n","spam = normaliser_url_email(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.5. Suppression des caractères spéciaux et les nombres\n","\n","    Nous avons utilisé la bibliothèque re pour supprimer les caractères spéciaux et les nombres."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def normaliser_nombre_char_spec(text_list : np.array):\n","    list_text = []\n","    for i, text in enumerate(text_list):\n","        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))# supprimer les ponctuations\n","        #replace every number with the word number\n","        num_pattern =  r\"\\d+\"\n","        text = re.sub(num_pattern, \" nombre \", text)\n","        text.replace(\"$\", \" dollar \")\n","        #supprimer les stop words\n","        stop = stopwords.words(\"english\")\n","        list = [m for m in text.split() if m not in stop]\n","        list_text.append(\" \".join(list))\n","        # list_text.append(text)\n","    return np.array(list_text)\n","eham = normaliser_nombre_char_spec(eham)\n","hham = normaliser_nombre_char_spec(hham)\n","spam  = normaliser_nombre_char_spec(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.5. Radicalisation des mots\n","\n","    Nous avons utilisé la bibliothèque nltk.PorterStemmer pour radicaliser les mots."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["250\n","2551\n","1397\n"]}],"source":["def radicalisation(text_list: np.array):\n","    list_radical = []\n","    # stemmer = PorterStemmer()\n","    stemmer = WordNetLemmatizer()\n","    for i, text in enumerate(text_list):\n","        list_mot = [stemmer.lemmatize(m) for m in text.split()]\n","        list_radical.append(\" \".join(list_mot))\n","    return np.array(list_radical)\n","\n","# text = \"\"\"\n","# > Anyone knows how much it costs to host a web portal ?\n","# >\n","# Well, it depends on how many visitors youre expecting. anywhere from less than 10 bucks a month to a couple of $100. You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 if youre running something big..\n","# To unsubscribe yourself from this mailing list, send an email to: groupname-unsubscribe@egroups.com\n","# \"\"\"\n","# ar = np.array([text])\n","# ar = minuscule(ar)\n","# ar = normaliser_url_email(ar)\n","# ar = normaliser_nombre_char_spec(ar)\n","# ar = radicalisation(ar)\n","# print(ar[0])\n","hham = radicalisation(hham)\n","eham = radicalisation(eham)\n","spam = radicalisation(spam)\n","print(len(hham))\n","print(len(eham))\n","print(len(spam))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##  4. Construction du vocabulaire\n","\n","    Nous avons utilisé la bibliothèque nltk.FreqDist pour construire le vocabulaire."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.1. regroupement des mots\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["data = np.concatenate((hham, eham, spam), axis=0)\n","\n","#save the data in a file\n","# with open('data.txt', 'w') as f:\n","#     for item in data:\n","#         f.write(\"%s,\" % item)\n","\n","#save the labels in a file\n","\n","labels = np.concatenate((np.zeros(len(hham)), np.ones(len(eham)), np.ones(len(spam))), axis=0)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["44927\n"]}],"source":["words = []\n","for line in data:\n","   for word in line.split():\n","       words.append(word)\n","words = set(words)\n","print(len(words)) "]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[('naveen', 1), ('ineptly', 1), ('port', 1), ('wirelessno', 1), ('radiouserland', 1), ('abit', 1), ('landspeed', 1), ('disenfranchised', 1), ('sona', 1), ('hdfrl', 1)]\n"]},{"data":{"text/plain":["30000"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["#get the most frequent words using the Counter class\n","word_counts = Counter(words)\n","print(word_counts.most_common(10))\n","most_common_words = word_counts.most_common(30000)\n","len(most_common_words)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["#store the most frequent words in a file\n","pd.DataFrame(most_common_words).to_csv(\"most_common_words.csv\", index=False)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.2. symbolisation des mots\n","\n","    Nous avons utilisé la bibliothèque tensorflow.keras.preprocessing.text.Tokenizer pour symboliser les mots."]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["\n","#reading the data from the file\n","data = np.genfromtxt('most_common_words.csv', delimiter=',', dtype=str)\n","#get the first column\n","data = data[:,0]\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=30000, oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(data)\n","word_index = tokenizer.word_index\n","sequences = tokenizer.texts_to_sequences(data)\n","padded = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=1000, padding='post', truncating='post')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.3. sauvegarde du vocabulaire dans un fichier csv"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["#store the words and their index in a file, the first column is the word and the second column is the index\n","import csv\n","with open('word_index.csv', 'w') as csv_file:\n","    writer = csv.writer(csv_file)\n","    for key, value in word_index.items():\n","        writer.writerow([key, value])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
