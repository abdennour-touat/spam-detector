{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Projet AARN"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1. Importation des librairies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import re\n","from bs4 import BeautifulSoup\n","import email\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","import string\n","from nltk.corpus import stopwords"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2. Importation des données"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["eham_folder_path = \"./data/easy_ham/\"\n","hham_folder_path = \"./data/hard_ham/\"\n","spam_folder_path = \"./data/spam_2/\"\n","\n","def getEmails(folder_path: str):\n","    data = []\n","    for fileName in os.listdir(folder_path):\n","        file =open(folder_path+fileName, encoding = \"ISO-8859-1\")\n","        text =file.read()\n","        data.append(text)\n","        file.close()\n","    return np.array(data)\n","eham = getEmails(eham_folder_path)\n","hham = getEmails(hham_folder_path)\n","spam = getEmails(spam_folder_path)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3. traitement des données (preprocessing)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.1. prendre le corps du mail \n","\n","    en utilisant la bibliothèque email, nous pouvons analyser l'email et en obtenir le corps (ce dont nous avons besoin)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_body(text_list: np.array):\n","    list_body = []\n","    for i, text in enumerate(text_list):\n","        text = email.message_from_string(text)\n","        if text.is_multipart():\n","            for part in text.walk():\n","                ctype = part.get_content_type()\n","                cdispo = str(part.get('Content-Disposition'))\n","                    # skip any text/plain (txt) attachments\n","                if ctype == 'text/plain' and 'attachment' not in cdispo:\n","                    body = part.get_payload(decode=True)  # get body of email\n","                    break\n","        else :\n","            body = text.get_payload(decode=True)\n","        list_body.append(body)\n","    return np.array(list_body)\n","eham = get_body(eham)\n","hham = get_body(hham)\n","spam = get_body(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.2. Suppression des balises HTML\n","\n","    Nous avons utilisé la bibliothèque beatifulsoup pour analyser la structure html (si elle existe) du corps et supprimer toutes les balises html."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def suprimer_HTML(text_list: np.array):\n","    list_html = []\n","    for i, text in enumerate(text_list):\n","        soup = BeautifulSoup(text, 'html.parser')\n","        list_html.append(soup.get_text())\n","    return np.array(list_html)\n","eham = suprimer_HTML(eham)\n","hham = suprimer_HTML(hham)\n","spam = suprimer_HTML(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.3. Minusculisation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def minuscule(text_list: np.array):\n","    list_mins = []\n","    for i, text in enumerate(text_list):\n","        list_mins.append( text.lower())\n","    return np.array(list_mins)\n","eham = minuscule(eham)\n","hham = minuscule(hham)\n","spam = minuscule(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.4. Elimination des URLs et des adresses email\n","\n","    Nous avons utilisé la bibliothèque re pour supprimer les URLs et les adresses email."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def normaliser_url_email(text_list: np.array):\n","    text_url = []\n","    for i, text in enumerate(text_list):\n","        #supprimer les emails\n","        text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', 'emailaddr ', text)\n","        #supprimer les urls\n","        # link_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n","        # link_pattern = r'^https[s]?:\\/\\/.*[\\r\\n]*'\n","        link_pattern=r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))';\n","        text = re.sub(link_pattern, 'httpaddr ', text)\n","        text_url.append(text)\n","    return np.array(text_url)\n","eham = normaliser_url_email(eham)\n","hham = normaliser_url_email(hham)\n","spam = normaliser_url_email(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.5. Suppression des caractères spéciaux et les nombres\n","\n","    Nous avons utilisé la bibliothèque re pour supprimer les caractères spéciaux et les nombres."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def normaliser_nombre_char_spec(text_list : np.array):\n","    list_text = []\n","    for i, text in enumerate(text_list):\n","        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))# supprimer les ponctuations\n","        #replace every number with the word number\n","        num_pattern =  r\"\\d+\"\n","        text = re.sub(num_pattern, \"nombre \", text)\n","        text.replace(\"$\", \"dollar \")\n","        #supprimer les stop words\n","        stop = stopwords.words(\"english\")\n","        list = [m for m in text.split() if m not in stop]\n","        list_text.append(\" \".join(list))\n","    return np.array(list_text)\n","eham = normaliser_nombre_char_spec(eham)\n","hham = normaliser_nombre_char_spec(hham)\n","spam  = normaliser_nombre_char_spec(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.5. Radicalisation des mots\n","\n","    Nous avons utilisé la bibliothèque nltk.PorterStemmer pour radicaliser les mots."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def radicalisation(text_list: np.array):\n","    list_radical = []\n","    # stemmer = PorterStemmer()\n","    stemmer = WordNetLemmatizer()\n","    for i, text in enumerate(text_list):\n","        list_mot = [stemmer.lemmatize(m) for m in text.split()]\n","        list_radical.append(\" \".join(list_mot))\n","    return np.array(list_radical)\n","\n","# text = \"\"\"\n","# > Anyone knows how much it costs to host a web portal ?\n","# >\n","# This can be\n","# Well, it depends on how many visitors youre expecting. anywhere from less than 10 bucks a month to a couple of $100. You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 if youre running something big..\n","# To unsubscribe yourself from this mailing list, send an email to: groupname-unsubscribe@egroups.com\n","# \"\"\"\n","# ar = np.array([text])\n","# ar = minuscule(ar)\n","# ar = normaliser_url_email(ar)\n","# ar = normaliser_nombre_char_spec(ar)\n","# ar = radicalisation(ar)\n","# print(ar[0])\n","hham = radicalisation(hham)\n","eham = radicalisation(eham)\n","spam = radicalisation(spam)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
