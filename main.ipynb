{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Projet AARN"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1. Importation des librairies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import re\n","from bs4 import BeautifulSoup\n","import email\n","import nltk\n","# nltk.download('stopwords')\n","# nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","import string\n","from nltk.corpus import stopwords\n","import tensorflow as tf\n","#import CounterVectorizer\n","#import Counter class\n","from collections import Counter\n","\n","from sklearn.feature_extraction.text import CountVectorizer"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2. Importation des données"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["eham_folder_path = \"./data/easy_ham/\"\n","hham_folder_path = \"./data/hard_ham/\"\n","spam_folder_path = \"./data/spam_2/\"\n","\n","def getEmails(folder_path: str):\n","    data = []\n","    for fileName in os.listdir(folder_path):\n","        file =open(folder_path+fileName, encoding = \"ISO-8859-1\")\n","        text =file.read()\n","        data.append(text)\n","        file.close()\n","    return np.array(data)\n","eham = getEmails(eham_folder_path)\n","hham = getEmails(hham_folder_path)\n","spam = getEmails(spam_folder_path)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3. traitement des données (preprocessing)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.1. prendre le corps du mail \n","\n","    en utilisant la bibliothèque email, nous pouvons analyser l'email et en obtenir le corps (ce dont nous avons besoin)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_body(text_list: np.array):\n","    list_body = []\n","    for i, text in enumerate(text_list):\n","        text = email.message_from_string(text)\n","        if text.is_multipart():\n","            for part in text.walk():\n","                ctype = part.get_content_type()\n","                cdispo = str(part.get('Content-Disposition'))\n","                    # skip any text/plain (txt) attachments\n","                if ctype == 'text/plain' and 'attachment' not in cdispo:\n","                    body = part.get_payload(decode=True)  # get body of email\n","                    break\n","        else :\n","            body = text.get_payload(decode=True)\n","        list_body.append(body)\n","    return np.array(list_body)\n","eham = get_body(eham)\n","hham = get_body(hham)\n","spam = get_body(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.2. Suppression des balises HTML\n","\n","    Nous avons utilisé la bibliothèque beatifulsoup pour analyser la structure html (si elle existe) du corps et supprimer toutes les balises html."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def suprimer_HTML(text_list: np.array):\n","    list_html = []\n","    for i, text in enumerate(text_list):\n","        soup = BeautifulSoup(text, 'html.parser')\n","        list_html.append(soup.get_text())\n","    return np.array(list_html)\n","eham = suprimer_HTML(eham)\n","hham = suprimer_HTML(hham)\n","spam = suprimer_HTML(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.3. Minusculisation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def minuscule(text_list: np.array):\n","    list_mins = []\n","    for i, text in enumerate(text_list):\n","        list_mins.append( text.lower())\n","    return np.array(list_mins)\n","eham = minuscule(eham)\n","hham = minuscule(hham)\n","spam = minuscule(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.4. Elimination des URLs et des adresses email\n","\n","    Nous avons utilisé la bibliothèque re pour supprimer les URLs et les adresses email."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def normaliser_url_email(text_list: np.array):\n","    text_url = []\n","    for i, text in enumerate(text_list):\n","        #supprimer les emails\n","        text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', 'emailaddr ', text)\n","        #supprimer les urls\n","        link_pattern=r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))';\n","        text = re.sub(link_pattern, 'httpaddr ', text)\n","        text_url.append(text)\n","    return np.array(text_url)\n","eham = normaliser_url_email(eham)\n","hham = normaliser_url_email(hham)\n","spam = normaliser_url_email(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.5. Suppression des caractères spéciaux et les nombres\n","\n","    Nous avons utilisé la bibliothèque re pour supprimer les caractères spéciaux et les nombres."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def normaliser_nombre_char_spec(text_list : np.array):\n","    list_text = []\n","    for i, text in enumerate(text_list):\n","        text = text.translate(str.maketrans(\"\", \"\", string.punctuation))# supprimer les ponctuations\n","        #replace every number with the word number\n","        num_pattern =  r\"\\d+\"\n","        text = re.sub(num_pattern, \" nombre \", text)\n","        text.replace(\"$\", \" dollar \")\n","        #supprimer les stop words\n","        stop = stopwords.words(\"english\")\n","        list = [m for m in text.split() if m not in stop]\n","        list_text.append(\" \".join(list))\n","        # list_text.append(text)\n","    return np.array(list_text)\n","eham = normaliser_nombre_char_spec(eham)\n","hham = normaliser_nombre_char_spec(hham)\n","spam  = normaliser_nombre_char_spec(spam)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 3.5. Radicalisation des mots\n","\n","    Nous avons utilisé la bibliothèque nltk.PorterStemmer pour radicaliser les mots."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def radicalisation(text_list: np.array):\n","    list_radical = []\n","    # stemmer = PorterStemmer()\n","    stemmer = WordNetLemmatizer()\n","    for i, text in enumerate(text_list):\n","        list_mot = [stemmer.lemmatize(m) for m in text.split()]\n","        list_radical.append(\" \".join(list_mot))\n","    return np.array(list_radical)\n","\n","# text = \"\"\"\n","# > Anyone knows how much it costs to host a web portal ?\n","# >\n","# Well, it depends on how many visitors youre expecting. anywhere from less than 10 bucks a month to a couple of $100. You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 if youre running something big..\n","# To unsubscribe yourself from this mailing list, send an email to: groupname-unsubscribe@egroups.com\n","# \"\"\"\n","# ar = np.array([text])\n","# ar = minuscule(ar)\n","# ar = normaliser_url_email(ar)\n","# ar = normaliser_nombre_char_spec(ar)\n","# ar = radicalisation(ar)\n","# print(ar[0])\n","hham = radicalisation(hham)\n","eham = radicalisation(eham)\n","spam = radicalisation(spam)\n","print(len(hham))\n","print(len(eham))\n","print(len(spam))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["##  4. Construction du vocabulaire\n","\n","    Nous avons utilisé la bibliothèque nltk.FreqDist pour construire le vocabulaire."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.1. regroupement des mots\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data = np.concatenate((hham, eham, spam), axis=0)\n","\n","labels = np.concatenate((np.zeros(len(hham)), np.ones(len(eham)), np.ones(len(spam))), axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# write each data with its label in a file\n","with open(\"data.csv\", \"w\") as f:\n","    for i, text in enumerate(data):\n","        f.write(text + \",\" + str(int(labels[i])) + \"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["words = []\n","# for line in data:\n","for line in spam:\n","    for mot in line.split():\n","        words.append(mot)\n","# words = (words)\n","print(len(words)) "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.DataFrame(words).head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wordCount = Counter(words)\n","mostCommon = [word for word, count in wordCount.items() if count >= 4 ]\n","print(mostCommon)\n","\n","len(mostCommon)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#save the most common words with a token in a csv file\n","with open(\"vocab.csv\", \"w\") as f:\n","    for i, word in enumerate(mostCommon):\n","        f.write(word + \",\" + str(i) + \"\\n\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.2. Extraction des caractéristiques\n","\n","on va utiliser la representation par comptage des mots"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tech update today vital sign july nombre nombr...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>view newsletter fullcolor visit httpaddr mediu...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>today headline register unsubscribe daily news...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>cnet shopper newsletter mac edition shopper cn...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>testing patch top today cv patch didnt help fo...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  label\n","0  tech update today vital sign july nombre nombr...      0\n","1  view newsletter fullcolor visit httpaddr mediu...      0\n","2  today headline register unsubscribe daily news...      0\n","3  cnet shopper newsletter mac edition shopper cn...      0\n","4  testing patch top today cv patch didnt help fo...      0"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["mails = pd.read_csv(\"data.csv\", header=None)\n","mails.columns = [\"text\", \"label\"]\n","mails.dropna(inplace=True)\n","mails.head()"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["#read the vocab file\n","vocab = pd.read_csv(\"vocab.csv\", header=None)\n","vocab.columns = [\"word\", \"token\"]\n","vocab.head()\n","vocab_words = vocab[\"word\"].values\n","vocab_tokens = vocab[\"token\"].values"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>tech update today vital sign july nombre nombr...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>view newsletter fullcolor visit httpaddr mediu...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>today headline register unsubscribe daily news...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>cnet shopper newsletter mac edition shopper cn...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>testing patch top today cv patch didnt help fo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                   0\n","0  tech update today vital sign july nombre nombr...\n","1  view newsletter fullcolor visit httpaddr mediu...\n","2  today headline register unsubscribe daily news...\n","3  cnet shopper newsletter mac edition shopper cn...\n","4  testing patch top today cv patch didnt help fo..."]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["y = np.array(mails[\"label\"])\n","X = np.array(mails[\"text\"])\n","\n","pd.DataFrame(X).head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# generate the caracteristic matrix\n","def generate_caracteristic_matrix(X, vocab):\n","    X_matrix = np.zeros((len(X), len(vocab)))\n","    for i, text in enumerate(X):\n","        words = text.split()\n","        for j, voc in enumerate(vocab):\n","            if voc in words:\n","                X_matrix[i, j] = 1\n","    return X_matrix\n","\n"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["# extract chracteristic matrix using sklearn\n","def gen_carcateristic(X, vocab):\n","    vectorizer = CountVectorizer(vocabulary=vocab)\n","    X_matrix = vectorizer.fit_transform(X)\n","    return X_matrix"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 5. Classification des emails\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 5.1. avec sklearn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 5.2. avec un reseau de neurones\n","\n","    Nous avons utilisé la bibliothèque tensorflow.keras pour construire un reseau de neurones."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
